{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7c44ff",
   "metadata": {},
   "source": [
    "# Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5255cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to remove the unwanted charaters\n",
    "import re\n",
    "\n",
    "# Pandas to store the dataset and make the necessary operations for EDA\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib to visualize our data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# NLTK tools to help clean the string for the model to train\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Tfdf vectorizer to convert the words into vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# To split the data for train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metrics to measure the performance and accuracy for the model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "# Calculate the model running time\n",
    "import time\n",
    "\n",
    "# Use counter to calculated the most common words\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78248803",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6685cd0",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d0379",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55499086",
   "metadata": {},
   "source": [
    "### Drop the unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d228e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spam_df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff1b617-06db-4151-8f33-62350965b9e6",
   "metadata": {},
   "source": [
    "### Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c1927-0e17-4218-8d75-aa7d4d9cf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'v1': 'target', 'v2': 'text'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987e9514-8c7a-440a-a2e2-de8dadf171b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check for duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ccd701-b185-471d-bb1b-60224ff84563",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d6ac7-52d8-4a05-b1ef-1752c94b4e43",
   "metadata": {},
   "source": [
    "### Remove the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b776fc-1ab1-4117-a3ed-a17b2ba5df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(keep='first')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d2041-e2d7-4781-95bf-61f486c14dca",
   "metadata": {},
   "source": [
    "### Convert the target column into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0138237-6178-4f00-9918-6fa7ba8740c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df['target'] = LabelEncoder().fit_transform(df['target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a553c1-df3f-425b-a288-298ed2874f3e",
   "metadata": {},
   "source": [
    "### Count the total number of spam and not spam text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d238e64-c02f-45af-9a31-6ffdcac082d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ad262-f15a-4b39-8445-a2e1a95bd251",
   "metadata": {},
   "source": [
    "### Lets visualize the spam count in pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d1361-02a5-44ce-87ca-cf9855608262",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df['target'].value_counts(), labels=['not spam', 'spam'], autopct='%0.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733cd40e-2c74-473d-9558-758efe623121",
   "metadata": {},
   "source": [
    "### Lets create a new column to keep the number of characters in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94316f4b-b7f0-4d83-bec7-fac71553cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_characters'] = df['text'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41376462-7695-41c0-9324-31736d8a4bfc",
   "metadata": {},
   "source": [
    "### Lets create a new column to keep the number of words in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8db1e-a9ef-48ee-bb7a-490f836d91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df['text'].apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77688717-4cf7-49d7-b06b-6262b428ba77",
   "metadata": {},
   "source": [
    "### Lets create a new column to keep the number of sentences in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43802f8-db52-49b1-b6bb-a9bb0afcc5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_sentence'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d241449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifier():\n",
    "    def __init__(self, X, y, max_features):\n",
    "        self.max_features = max_features\n",
    "        \n",
    "        X_vec = self.transform_and_vectorize(X);\n",
    "        self.plot_most_common_words();\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_vec, y, \n",
    "                                                                                test_size=0.2, \n",
    "                                                                                random_state=42)\n",
    "        \n",
    "    # Clean the feature column\n",
    "    def transform_text(self, text, method, method_name):\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = text.split()\n",
    "        words = []\n",
    "        if method_name == 'stemmer':\n",
    "            words = [method.stem(word) for word in text if word not in stopwords.words('english') and word not in string.punctuation]\n",
    "        else:\n",
    "            words = [method.lemmatize(word) for word in text if word not in stopwords.words('english') and word not in string.punctuation]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    # Convert the feature column into vectors\n",
    "    def data_into_vector(self, X):\n",
    "        tfidf = TfidfVectorizer(max_features=self.max_features)\n",
    "        return tfidf.fit_transform(X)\n",
    "    \n",
    "    # Clean the feature using portal stemming\n",
    "    def using_stemming(self, X):\n",
    "        stemmer = PorterStemmer()\n",
    "        return X.apply(lambda x: self.transform_text(x, stemmer, 'stemmer'))\n",
    "    \n",
    "    # Clean the feature using snowball stemming\n",
    "    def using_stemming(self, X):\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        return X.apply(lambda x: self.transform_text(x, stemmer, 'stemmer'))\n",
    "    \n",
    "    # Clean the feature using lemmatization\n",
    "    def using_lemmatization(self, X):\n",
    "        lemma = WordNetLemmatizer()\n",
    "        return X.apply(lambda x: self.transform_text(x, lemma, 'lemma'))\n",
    "    \n",
    "    # Transform and vectorize the text\n",
    "    def transform_and_vectorize(self, X):\n",
    "        df['transform_text'] = self.using_stemming(X)\n",
    "        return self.data_into_vector(df['transform_text'])\n",
    "    \n",
    "    # Count the total number of words in the list of text\n",
    "    def calculate_the_total_words(self, target):\n",
    "        corpus = []\n",
    "        for text in df[df['target'] == target]['transform_text'].tolist():\n",
    "            for word in text.split():\n",
    "                corpus.append(word)\n",
    "        return corpus\n",
    "    \n",
    "    # Plot barplot from corpus\n",
    "    def plot_barplot(self, corpus, x_label):\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        sns.barplot(pd.DataFrame(Counter(corpus).most_common(30))[0], pd.DataFrame(Counter(corpus).most_common(30))[1])\n",
    "        plt.xticks(rotation='vertical')\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel('Number of words')\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot the graph for most common words in the data for spam and not spam text\n",
    "    def plot_most_common_words(self):\n",
    "        corpus_spam = self.calculate_the_total_words(0)\n",
    "        corpus_not_spam = self.calculate_the_total_words(1)\n",
    "        \n",
    "        self.plot_barplot(corpus_spam, 'Most common non spam words')\n",
    "        self.plot_barplot(corpus_not_spam, 'Most common spam words')\n",
    "    \n",
    "    # Training the model\n",
    "    def model_train(self, X_train, y_train, classifier):\n",
    "        print(classifier)\n",
    "        return classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Prediction from model\n",
    "    def model_prediction(self, model, X_test):\n",
    "        return model.predict(X_test)\n",
    "    \n",
    "    # Count number of prediction are true or false using confusion matrix\n",
    "    def model_confusion_matrix(self, y_test, y_pred):\n",
    "        return confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Model accuracy score\n",
    "    def model_accuracy(self, y_test, y_pred):\n",
    "        return accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Model precision score\n",
    "    def model_precision(self, y_test, y_pred):\n",
    "        return precision_score(y_test, y_pred)\n",
    "        \n",
    "    # Train the model and check the performance\n",
    "    def train_check_performance(self, classifier):\n",
    "        model = self.model_train(self.X_train, self.y_train, classifier)\n",
    "        y_pred = self.model_prediction(model, self.X_test)\n",
    "\n",
    "        confusion_mat = self.model_confusion_matrix(self.y_test, y_pred)\n",
    "        accuracy = self.model_accuracy(self.y_test, y_pred)\n",
    "        precision = self.model_precision(self.y_test, y_pred)\n",
    "\n",
    "        return (model, y_pred, confusion_mat, accuracy, precision)\n",
    "    \n",
    "    # Call this to train model\n",
    "    def run_classifier(self, classifier):\n",
    "        return self.train_check_performance(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e2904-bd30-437c-b100-3f361905f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_matrix = []\n",
    "def run_classifier_check_performance(spam_classifier, classifier):\n",
    "    model, y_pred, confusion_mat, accuracy, precision = spam_classifier.run_classifier(classifier)\n",
    "    performance_matrix.append({'Classifier': classifier, 'Accuracy': accuracy, 'Precision': precision})\n",
    "    print(\"==============================================================\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"==============================================================\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"==============================================================\")\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"==============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2379e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_classifier = SpamClassifier(df['text'], df['target'], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a84b0-0a83-4659-a46f-1fa981f3cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7913230-0484-4c4f-81fb-b64207bd21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "# xgb = XGBClassifier(n_estimators=50,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c1377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [svc, knc, bnb, mnb, dtc, lrc, rfc, abc, bc, etc, gbdt] # GaussianNB() will not work on the lot of sparse data (contains a lot of zeros) it required the dense data to work properly\n",
    "for classifier in classifiers:\n",
    "    run_classifier_check_performance(spam_classifier, classifier)\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a952d-4bbd-4886-a972-c2099681e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = pd.DataFrame(performance_matrix)\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d8f43-0591-425a-934c-d94cc4debd93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
